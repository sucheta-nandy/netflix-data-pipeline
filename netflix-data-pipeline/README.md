# ğŸ¬ Netflix Data Engineering Pipeline

A production-grade ETL pipeline that ingests raw Netflix metadata, performs cleaning and transformation using Python and Apache Spark, and loads optimized parquet files into a query-ready data warehouse with automated validation at every stage.

![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![PySpark](https://img.shields.io/badge/PySpark-3.5-orange?logo=apache-spark)
![License](https://img.shields.io/badge/License-MIT-green)

## ğŸŒŸ Key Features

### Modular Workflow Design
- **Independent Components**: Ingestion, transformation, validation, and warehouse layers
- **Scalable Architecture**: Easily handles growing datasets
- **Reusable Modules**: Plug-and-play components for different data sources

### Automated Validation Layer
- **Schema Validation**: Ensures data structure integrity
- **Quality Checks**: Completeness, uniqueness, value ranges
- **Business Rules**: Domain-specific validations
- **Automated Reports**: JSON reports with detailed metrics

### Apache Spark Processing
- **Distributed Transformations**: Handles large-scale data processing
- **Optimized Operations**: Broadcast joins, caching, partitioning
- **Advanced Enrichment**: Genre parsing, duration extraction, categorization

### Parquet Data Warehouse
- **Columnar Storage**: Optimized for analytics queries
- **Partitioning Strategy**: By release_year and type
- **Snappy Compression**: Reduced storage footprint
- **Schema Evolution**: Supports schema changes over time

## ğŸ“ Project Structure

```
netflix-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw input data
â”‚   â”œâ”€â”€ staging/                # Intermediate data
â”‚   â”œâ”€â”€ warehouse/              # Parquet files (partitioned)
â”‚   â””â”€â”€ reports/                # Validation reports
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/              # Data loading modules
â”‚   â”œâ”€â”€ transformation/         # Cleaning & Spark jobs
â”‚   â”œâ”€â”€ validation/             # Validation framework
â”‚   â”œâ”€â”€ warehouse/              # Parquet writer
â”‚   â”œâ”€â”€ orchestration/          # Pipeline orchestrator
â”‚   â””â”€â”€ utils/                  # Logging & Spark utilities
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml    # Pipeline configuration
â”œâ”€â”€ tests/                      # Unit & integration tests
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ run_pipeline.py             # Main entry point
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### 2. Run the Pipeline

```bash
# Run with default settings
python run_pipeline.py

# Run with custom input
python run_pipeline.py --input data/raw/netflix_titles.csv --table netflix_content

# Run with custom config
python run_pipeline.py --config config/pipeline_config.yaml
```

### 3. Check the Results

```bash
# View warehouse structure
ls -R data/warehouse/

# View validation reports
cat data/reports/validation_report_*.json
```

## ğŸ“Š Pipeline Stages

### Stage 1: Data Ingestion
- Loads raw Netflix metadata (CSV/JSON/Parquet)
- Auto-detects file format and encoding
- Handles compressed files
- **Output**: Raw Spark DataFrame

### Stage 2: Raw Data Validation
- Schema validation (column types, nullability)
- Completeness checks (required fields)
- Uniqueness validation (show_id)
- Value range checks (release_year)
- **Output**: Validation report (JSON)

### Stage 3: Data Transformation
**Cleaning**:
- Remove duplicates
- Handle missing values
- Standardize dates
- Normalize text fields

**Spark Transformations**:
- Extract release_year from dates
- Parse duration into value + unit
- Split genres into arrays
- Normalize country codes
- Categorize ratings by age group
- Calculate content age
- Add recency flags

**Output**: Transformed DataFrame with 18+ columns

### Stage 4: Transformed Data Validation
- Schema validation (transformed schema)
- Business rule validation
- Statistical checks
- Allowed values verification
- **Output**: Validation report (JSON)

### Stage 5: Load to Warehouse
- Write to Parquet format
- Partition by `release_year` and `type`
- Snappy compression
- **Output**: Partitioned parquet files

### Stage 6: Reporting
- Generate pipeline summary
- Log execution metrics
- Save validation reports

## ğŸ”§ Configuration

Edit `config/pipeline_config.yaml`:

```yaml
# Spark settings
spark:
  app_name: NetflixETL
  master: local[*]
  executor_memory: 4g
  driver_memory: 2g

# Validation settings
validation:
  fail_on_error: false      # Continue on validation errors
  max_error_rate: 0.01      # Max 1% error rate
  generate_reports: true

# Warehouse settings
warehouse:
  format: parquet
  compression: snappy
  partition_by: [release_year, type]
```

## ğŸ“ˆ Data Schema

### Input Schema (Raw)
- `show_id`: Unique identifier
- `type`: Movie or TV Show
- `title`: Content title
- `director`: Director name(s)
- `cast`: Cast members
- `country`: Production country
- `date_added`: Date added to Netflix
- `release_year`: Release year
- `rating`: Content rating (G, PG, R, etc.)
- `duration`: Duration string
- `listed_in`: Genres (comma-separated)
- `description`: Content description

### Output Schema (Transformed)
All input columns plus:
- `primary_country`: First country from list
- `duration_value`: Numeric duration
- `duration_unit`: 'min' or 'Season'
- `genres`: Array of genres
- `genre_count`: Number of genres
- `age_category`: Kids/Teens/Adults
- `content_age`: Years since release
- `is_recent`: Boolean flag

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Run specific test
pytest tests/test_transformation.py

# Run with coverage
pytest --cov=src tests/
```

## ğŸ“Š Example Output

### Warehouse Structure
```
data/warehouse/netflix_content/
â”œâ”€â”€ release_year=2020/
â”‚   â”œâ”€â”€ type=Movie/
â”‚   â”‚   â””â”€â”€ part-00000.parquet
â”‚   â””â”€â”€ type=TV Show/
â”‚       â””â”€â”€ part-00000.parquet
â””â”€â”€ release_year=2021/
    â”œâ”€â”€ type=Movie/
    â”‚   â””â”€â”€ part-00000.parquet
    â””â”€â”€ type=TV Show/
        â””â”€â”€ part-00000.parquet
```

### Validation Report
```json
{
  "timestamp": "2024-12-01T23:30:00",
  "stage": "transformed",
  "summary": {
    "total_checks": 5,
    "passed_checks": 5,
    "failed_checks": 0,
    "success_rate": 1.0
  },
  "status": "PASSED"
}
```

## ğŸ¯ Key Achievements

âœ… **Modular Design**: Independent, reusable components  
âœ… **Automated Validation**: Quality checks at every stage  
âœ… **Scalable Processing**: Apache Spark for big data  
âœ… **Optimized Storage**: Parquet with partitioning  
âœ… **Production-Ready**: Logging, error handling, testing  
âœ… **Well-Documented**: Comprehensive README and code comments

## ğŸ” Querying the Warehouse

Use PySpark to query the warehouse:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Query").getOrCreate()

# Read parquet data
df = spark.read.parquet("data/warehouse/netflix_content")

# Query examples
df.filter("type = 'Movie'").count()
df.groupBy("age_category").count().show()
df.filter("is_recent = true").select("title", "release_year").show()
```

## ğŸ› ï¸ Tech Stack

- **Python 3.9+**: Core language
- **Apache Spark (PySpark 3.5)**: Distributed processing
- **Parquet**: Columnar storage format
- **PyYAML**: Configuration management
- **pytest**: Testing framework
- **pandas**: Data manipulation (ingestion)

## ğŸ“ Future Enhancements

- [ ] Add support for incremental loads
- [ ] Implement Change Data Capture (CDC)
- [ ] Add Airflow/Prefect orchestration
- [ ] Integrate with data catalog (e.g., AWS Glue)
- [ ] Add data lineage tracking
- [ ] Implement data quality dashboards

## ğŸ‘¨â€ğŸ’» Author

Created by Sucheta Nandy to demonstrate:
- Data engineering expertise
- Apache Spark proficiency
- Pipeline design and orchestration
- Data quality and validation
- Production-ready code practices

## ğŸ“§ Contact

For questions about this pipeline, please reach out through the Netflix application portal.

---

**Built with â¤ï¸ for Netflix | December 2025**
